{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3202adc",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 1430) (3453487636.py, line 1430)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 1430\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f\"   Clusters:        print(f\"Suggested optimal number of clusters: {optimal_k}\")\u001b[0m\n\u001b[0m                                                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 1430)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, OrdinalEncoder, RobustScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "import hdbscan\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class DemographicClusterer:\n",
    "    def __init__(self, data_path=None, df=None):\n",
    "        if df is not None:\n",
    "            self.df = df.copy()\n",
    "        else:\n",
    "            self.df = pd.read_csv(data_path)\n",
    "        \n",
    "        self.processed_data = None\n",
    "        self.cluster_results = {}\n",
    "    def create_composite_features(self):\n",
    "        \"\"\"\n",
    "        Create composite features for better clustering\n",
    "        \"\"\"\n",
    "        print(\"Creating composite features...\")\n",
    "        \n",
    "        # Professional categorization\n",
    "        profession_mapping = {\n",
    "            'Doctor': 'High-Skill', 'Engineer': 'High-Skill', 'Executive': 'High-Skill', \n",
    "            'Lawyer': 'High-Skill',\n",
    "            'Artist': 'Creative', 'Entertainment': 'Creative',\n",
    "            'Healthcare': 'Service/Care', 'Homemaker': 'Service/Care',\n",
    "            'Marketing': 'Business-Oriented'\n",
    "        }\n",
    "        \n",
    "        if 'profession' in self.df.columns:\n",
    "            self.df['profession_category'] = self.df['profession'].map(profession_mapping)\n",
    "        \n",
    "        # Life Stage Index\n",
    "        if all(col in self.df.columns for col in ['age', 'work_experience']):\n",
    "            def determine_life_stage(row):\n",
    "                age = row.get('age', 0)\n",
    "                exp = row.get('work_experience', 0)\n",
    "                marital = row.get('marital_status', 'Unknown')\n",
    "                \n",
    "                if age < 25:\n",
    "                    return 'Career Starter'\n",
    "                elif 22 <= age <= 35 and marital in ['Single', 'Unmarried'] and exp <= 3:\n",
    "                    return 'Young Professional'\n",
    "                elif 30 <= age <= 50 and marital == 'Married' and exp > 3:\n",
    "                    return 'Established Adult'\n",
    "                elif age >= 60:\n",
    "                    return 'Senior Segment'\n",
    "                else:\n",
    "                    return 'Transitional'\n",
    "            \n",
    "            self.df['life_stage'] = self.df.apply(determine_life_stage, axis=1)\n",
    "        \n",
    "        # Career Development Score (0-100 scale)\n",
    "        if all(col in self.df.columns for col in ['age', 'work_experience']):\n",
    "            education_weights = {'High School': 1, 'Bachelor': 2, 'Master': 3, 'PhD': 4}\n",
    "            \n",
    "            age_score = np.clip((self.df.get('age', 0) - 18) / 47 * 30, 0, 30)  # Max 30 points\n",
    "            exp_score = np.clip(self.df.get('work_experience', 0) / 20 * 40, 0, 40)  # Max 40 points\n",
    "            \n",
    "            if 'education' in self.df.columns:\n",
    "                edu_score = self.df['education'].map(education_weights).fillna(1) / 4 * 30  # Max 30 points\n",
    "            else:\n",
    "                edu_score = 15  # Default middle score\n",
    "            \n",
    "            self.df['career_development_score'] = age_score + exp_score + edu_score\n",
    "        \n",
    "        # Economic Capacity Index (0-100 scale)\n",
    "        if 'profession_category' in self.df.columns:\n",
    "            profession_weights = {'High-Skill': 4, 'Business-Oriented': 3, 'Creative': 2, 'Service/Care': 1}\n",
    "            prof_score = self.df['profession_category'].map(profession_weights).fillna(2) / 4 * 40\n",
    "            \n",
    "            if 'education' in self.df.columns:\n",
    "                edu_score = self.df['education'].map(education_weights).fillna(2) / 4 * 30\n",
    "            else:\n",
    "                edu_score = 15\n",
    "            \n",
    "            spending_weights = {'Low': 1, 'Average': 2, 'High': 3}\n",
    "            if 'spending_score' in self.df.columns:\n",
    "                spend_score = self.df['spending_score'].map(spending_weights).fillna(2) / 3 * 30\n",
    "            elif 'annual_spending' in self.df.columns:\n",
    "                # Normalize annual spending to 0-30 scale\n",
    "                spend_score = (self.df['annual_spending'] - self.df['annual_spending'].min()) / \\\n",
    "                             (self.df['annual_spending'].max() - self.df['annual_spending'].min()) * 30\n",
    "            else:\n",
    "                spend_score = 15\n",
    "            \n",
    "            self.df['economic_capacity_index'] = prof_score + edu_score + spend_score\n",
    "        \n",
    "        # Family Responsibility Factor\n",
    "        if 'age' in self.df.columns:\n",
    "            age_weight = np.clip(self.df['age'] / 70, 0.5, 1.5)  # Age factor\n",
    "            \n",
    "            marital_weight = 1.0\n",
    "            if 'marital_status' in self.df.columns:\n",
    "                marital_weights = {'Married': 1.5, 'Single': 0.8, 'Divorced': 1.2, 'Widowed': 1.0}\n",
    "                marital_weight = self.df['marital_status'].map(marital_weights).fillna(1.0)\n",
    "            \n",
    "            family_size = 1\n",
    "            if 'family_size' in self.df.columns:\n",
    "                family_size = self.df['family_size'].fillna(1)\n",
    "            \n",
    "            self.df['family_responsibility_factor'] = age_weight * marital_weight * family_size\n",
    "        \n",
    "        # Profession-Education Alignment\n",
    "        if all(col in self.df.columns for col in ['profession_category', 'education']):\n",
    "            def check_alignment(row):\n",
    "                prof_cat = row.get('profession_category', '')\n",
    "                education = row.get('education', '')\n",
    "                \n",
    "                if prof_cat == 'High-Skill' and education in ['Bachelor', 'Master', 'PhD']:\n",
    "                    return 1\n",
    "                elif prof_cat in ['Creative', 'Business-Oriented'] and education in ['Bachelor', 'Master']:\n",
    "                    return 1\n",
    "                elif prof_cat == 'Service/Care':\n",
    "                    return 1  # Service roles have varied education requirements\n",
    "                else:\n",
    "                    return 0\n",
    "            \n",
    "            self.df['profession_education_match'] = self.df.apply(check_alignment, axis=1)\n",
    "        \n",
    "        print(f\"Created composite features. New shape: {self.df.shape}\")\n",
    "        return self.df\n",
    "        \"\"\"\n",
    "        Handle mixed data types for clustering\n",
    "        \"\"\"\n",
    "        print(\"Starting data preprocessing...\")\n",
    "        \n",
    "        if categorical_cols is None:\n",
    "            categorical_cols = self.df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        if numerical_cols is None:\n",
    "            numerical_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        self.preprocessing_info['categorical_cols'] = categorical_cols\n",
    "        self.preprocessing_info['numerical_cols'] = numerical_cols\n",
    "        \n",
    "        # Create preprocessing pipeline\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', StandardScaler(), numerical_cols),\n",
    "                ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_cols)\n",
    "            ])\n",
    "        \n",
    "        self.processed_data = preprocessor.fit_transform(self.df)\n",
    "        self.preprocessor = preprocessor\n",
    "        \n",
    "        # Get feature names after preprocessing\n",
    "        num_features = numerical_cols\n",
    "        cat_features = []\n",
    "        if len(categorical_cols) > 0:\n",
    "            cat_features = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)\n",
    "        \n",
    "        self.feature_names = num_features + list(cat_features)\n",
    "        \n",
    "        print(f\"Data shape after preprocessing: {self.processed_data.shape}\")\n",
    "        print(f\"Features: {len(self.feature_names)} total\")\n",
    "        return self.processed_data\n",
    "    \n",
    "    def find_optimal_clusters(self, max_clusters=10):\n",
    "        \"\"\"\n",
    "        Use elbow method and silhouette analysis to find optimal number of clusters\n",
    "        \"\"\"\n",
    "        inertias = []\n",
    "        silhouette_scores = []\n",
    "        k_range = range(2, max_clusters + 1)\n",
    "        \n",
    "        for k in k_range:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "            labels = kmeans.fit_predict(self.processed_data)\n",
    "            \n",
    "            inertias.append(kmeans.inertia_)\n",
    "            sil_score = silhouette_score(self.processed_data, labels)\n",
    "            silhouette_scores.append(sil_score)\n",
    "        \n",
    "        # Plot elbow curve and silhouette scores\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        ax1.plot(k_range, inertias, 'bo-')\n",
    "        ax1.set_xlabel('Number of Clusters')\n",
    "        ax1.set_ylabel('Inertia')\n",
    "        ax1.set_title('Elbow Method')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        ax2.plot(k_range, silhouette_scores, 'ro-')\n",
    "        ax2.set_xlabel('Number of Clusters')\n",
    "        ax2.set_ylabel('Silhouette Score')\n",
    "        ax2.set_title('Silhouette Analysis')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Suggest optimal k\n",
    "        optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "        \n",
    "    def gap_statistic(self, max_clusters=10, n_refs=20):\n",
    "        \"\"\"\n",
    "        Calculate Gap Statistic for optimal cluster number determination\n",
    "        \"\"\"\n",
    "        print(\"Calculating Gap Statistic...\")\n",
    "        \n",
    "        gaps = []\n",
    "        results_df = []\n",
    "        \n",
    "        for k in range(1, max_clusters + 1):\n",
    "            # Cluster the data\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "            labels = kmeans.fit_predict(self.processed_data)\n",
    "            \n",
    "            # Calculate within-cluster sum of squares\n",
    "            wk = sum([np.sum((self.processed_data[labels == i] - kmeans.cluster_centers_[i])**2) \n",
    "                     for i in range(k)])\n",
    "            \n",
    "            # Generate reference datasets and calculate their within-cluster sum of squares\n",
    "            ref_wks = []\n",
    "            for _ in range(n_refs):\n",
    "                # Generate random reference data with same shape\n",
    "                ref_data = np.random.uniform(\n",
    "                    low=self.processed_data.min(axis=0),\n",
    "                    high=self.processed_data.max(axis=0),\n",
    "                    size=self.processed_data.shape\n",
    "                )\n",
    "                \n",
    "                ref_kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "                ref_labels = ref_kmeans.fit_predict(ref_data)\n",
    "                \n",
    "                ref_wk = sum([np.sum((ref_data[ref_labels == i] - ref_kmeans.cluster_centers_[i])**2) \n",
    "                             for i in range(k)])\n",
    "                ref_wks.append(ref_wk)\n",
    "            \n",
    "            # Calculate gap\n",
    "            gap = np.log(np.mean(ref_wks)) - np.log(wk)\n",
    "            gaps.append(gap)\n",
    "            \n",
    "            results_df.append({\n",
    "                'k': k,\n",
    "                'gap': gap,\n",
    "                'log_wk': np.log(wk),\n",
    "                'ref_log_wk': np.log(np.mean(ref_wks))\n",
    "            })\n",
    "        \n",
    "        # Find optimal k (first k where gap(k) >= gap(k+1) - s(k+1))\n",
    "        optimal_k = 1\n",
    "        for i in range(len(gaps) - 1):\n",
    "            if gaps[i] >= gaps[i + 1]:  # Simplified rule\n",
    "                optimal_k = i + 1\n",
    "                break\n",
    "        \n",
    "        # Plot gap statistic\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(1, max_clusters + 1), gaps, 'bo-')\n",
    "        plt.axvline(x=optimal_k, color='r', linestyle='--', alpha=0.7, label=f'Optimal k={optimal_k}')\n",
    "        plt.xlabel('Number of Clusters (k)')\n",
    "        plt.ylabel('Gap Statistic')\n",
    "        plt.title('Gap Statistic for Optimal Number of Clusters')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Gap Statistic suggests optimal k: {optimal_k}\")\n",
    "        return optimal_k, gaps\n",
    "    \n",
    "    def apply_clustering_methods(self, n_clusters=4):\n",
    "        \"\"\"\n",
    "        Apply enhanced clustering algorithms with HDBSCAN as primary method\n",
    "        \"\"\"\n",
    "        print(\"Applying enhanced clustering methods...\")\n",
    "        \n",
    "        methods = {}\n",
    "        \n",
    "        # 1. HDBSCAN (Primary) - Density-based clustering\n",
    "        print(\"Applying HDBSCAN...\")\n",
    "        hdbscan_clusterer = hdbscan.HDBSCAN(\n",
    "            min_cluster_size=max(5, len(self.processed_data) // 50),  # Adaptive min size\n",
    "            min_samples=3,\n",
    "            cluster_selection_epsilon=0.5,\n",
    "            cluster_selection_method='eom'\n",
    "        )\n",
    "        hdbscan_labels = hdbscan_clusterer.fit_predict(self.processed_data)\n",
    "        \n",
    "        methods['HDBSCAN'] = {\n",
    "            'model': hdbscan_clusterer,\n",
    "            'labels': hdbscan_labels,\n",
    "            'n_clusters': len(set(hdbscan_labels)) - (1 if -1 in hdbscan_labels else 0),\n",
    "            'noise_points': np.sum(hdbscan_labels == -1),\n",
    "            'cluster_probabilities': hdbscan_clusterer.probabilities_ if hasattr(hdbscan_clusterer, 'probabilities_') else None\n",
    "        }\n",
    "        \n",
    "        # 2. Gaussian Mixture Models (Secondary) - Probabilistic clustering\n",
    "        print(\"Applying Gaussian Mixture...\")\n",
    "        gmm = GaussianMixture(n_components=n_clusters, random_state=42, covariance_type='full')\n",
    "        gmm.fit(self.processed_data)\n",
    "        gmm_labels = gmm.predict(self.processed_data)\n",
    "        gmm_probs = gmm.predict_proba(self.processed_data)\n",
    "        \n",
    "        methods['Gaussian Mixture'] = {\n",
    "            'model': gmm,\n",
    "            'labels': gmm_labels,\n",
    "            'n_clusters': n_clusters,\n",
    "            'probabilities': gmm_probs,\n",
    "            'aic': gmm.aic(self.processed_data),\n",
    "            'bic': gmm.bic(self.processed_data)\n",
    "        }\n",
    "        \n",
    "        # 3. K-Means (Validation) - Traditional centroid-based\n",
    "        print(\"Applying K-Means...\")\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=20)\n",
    "        kmeans_labels = kmeans.fit_predict(self.processed_data)\n",
    "        \n",
    "        methods['K-Means'] = {\n",
    "            'model': kmeans,\n",
    "            'labels': kmeans_labels,\n",
    "            'n_clusters': n_clusters,\n",
    "            'inertia': kmeans.inertia_,\n",
    "            'cluster_centers': kmeans.cluster_centers_\n",
    "        }\n",
    "        \n",
    "        # 4. Hierarchical (Exploratory) - For understanding relationships\n",
    "        print(\"Applying Hierarchical Clustering...\")\n",
    "        hierarchical = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "        hierarchical_labels = hierarchical.fit_predict(self.processed_data)\n",
    "        \n",
    "        methods['Hierarchical'] = {\n",
    "            'model': hierarchical,\n",
    "            'labels': hierarchical_labels,\n",
    "            'n_clusters': n_clusters\n",
    "        }\n",
    "        \n",
    "        # Calculate comprehensive metrics for each method\n",
    "        for name, result in methods.items():\n",
    "            labels = result['labels']\n",
    "            \n",
    "            # Skip metrics calculation if only one cluster or all noise\n",
    "            if len(np.unique(labels)) <= 1 or (len(np.unique(labels)) == 2 and -1 in labels):\n",
    "                result.update({\n",
    "                    'silhouette_score': -1,\n",
    "                    'calinski_harabasz_score': 0,\n",
    "                    'davies_bouldin_score': float('inf'),\n",
    "                    'stability_score': 0\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            # Standard metrics\n",
    "            if len(np.unique(labels)) > 1:\n",
    "                # Handle noise points for HDBSCAN\n",
    "                if -1 in labels:\n",
    "                    mask = labels != -1\n",
    "                    if np.sum(mask) > 1 and len(np.unique(labels[mask])) > 1:\n",
    "                        sil_score = silhouette_score(self.processed_data[mask], labels[mask])\n",
    "                        ch_score = calinski_harabasz_score(self.processed_data[mask], labels[mask])\n",
    "                        db_score = davies_bouldin_score(self.processed_data[mask], labels[mask])\n",
    "                    else:\n",
    "                        sil_score = ch_score = db_score = -1\n",
    "                else:\n",
    "                    sil_score = silhouette_score(self.processed_data, labels)\n",
    "                    ch_score = calinski_harabasz_score(self.processed_data, labels)\n",
    "                    db_score = davies_bouldin_score(self.processed_data, labels)\n",
    "            else:\n",
    "                sil_score = ch_score = db_score = -1\n",
    "            \n",
    "            # Stability analysis using bootstrap sampling\n",
    "            stability_score = self.calculate_stability(result['model'], name)\n",
    "            \n",
    "            result.update({\n",
    "                'silhouette_score': sil_score,\n",
    "                'calinski_harabasz_score': ch_score,\n",
    "                'davies_bouldin_score': db_score,\n",
    "                'stability_score': stability_score\n",
    "            })\n",
    "        \n",
    "        self.cluster_results = methods\n",
    "        return methods\n",
    "    \n",
    "    def calculate_stability(self, model, method_name, n_bootstrap=10):\n",
    "        \"\"\"\n",
    "        Calculate clustering stability using bootstrap sampling\n",
    "        \"\"\"\n",
    "        try:\n",
    "            n_samples = len(self.processed_data)\n",
    "            stability_scores = []\n",
    "            \n",
    "            for _ in range(n_bootstrap):\n",
    "                # Bootstrap sample\n",
    "                indices = np.random.choice(n_samples, size=int(0.8 * n_samples), replace=True)\n",
    "                bootstrap_data = self.processed_data[indices]\n",
    "                \n",
    "                # Apply clustering to bootstrap sample\n",
    "                if method_name == 'HDBSCAN':\n",
    "                    labels = model.fit_predict(bootstrap_data)\n",
    "                elif method_name == 'Gaussian Mixture':\n",
    "                    temp_model = GaussianMixture(n_components=model.n_components, random_state=42)\n",
    "                    temp_model.fit(bootstrap_data)\n",
    "                    labels = temp_model.predict(bootstrap_data)\n",
    "                elif method_name == 'K-Means':\n",
    "                    temp_model = KMeans(n_clusters=model.n_clusters, random_state=42, n_init=10)\n",
    "                    labels = temp_model.fit_predict(bootstrap_data)\n",
    "                elif method_name == 'Hierarchical':\n",
    "                    temp_model = AgglomerativeClustering(n_clusters=model.n_clusters)\n",
    "                    labels = temp_model.fit_predict(bootstrap_data)\n",
    "                \n",
    "                # Calculate silhouette score if possible\n",
    "                if len(np.unique(labels)) > 1:\n",
    "                    if -1 in labels:  # Handle noise points\n",
    "                        mask = labels != -1\n",
    "                        if np.sum(mask) > 1 and len(np.unique(labels[mask])) > 1:\n",
    "                            score = silhouette_score(bootstrap_data[mask], labels[mask])\n",
    "                        else:\n",
    "                            score = -1\n",
    "                    else:\n",
    "                        score = silhouette_score(bootstrap_data, labels)\n",
    "                    stability_scores.append(score)\n",
    "            \n",
    "            return np.mean(stability_scores) if stability_scores else 0\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def evaluate_clusters(self):\n",
    "        \"\"\"\n",
    "        Enhanced cluster evaluation with business validation\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"COMPREHENSIVE CLUSTERING EVALUATION\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        evaluation_results = []\n",
    "        \n",
    "        for method, result in self.cluster_results.items():\n",
    "            labels = result['labels']\n",
    "            n_clusters = result['n_clusters']\n",
    "            \n",
    "            # Basic metrics\n",
    "            sil_score = result.get('silhouette_score', -1)\n",
    "            ch_score = result.get('calinski_harabasz_score', 0)\n",
    "            db_score = result.get('davies_bouldin_score', float('inf'))\n",
    "            stability = result.get('stability_score', 0)\n",
    "            \n",
    "            # Business validation metrics\n",
    "            cluster_sizes = np.bincount(labels[labels >= 0])  # Exclude noise points\n",
    "            min_cluster_size_pct = (np.min(cluster_sizes) / len(labels)) * 100 if len(cluster_sizes) > 0 else 0\n",
    "            \n",
    "            # Actionability score (based on feature separation)\n",
    "            actionability_score = self.calculate_actionability_score(labels)\n",
    "            \n",
    "            # Overall score (weighted combination)\n",
    "            if sil_score > 0:\n",
    "                overall_score = (\n",
    "                    0.3 * sil_score +\n",
    "                    0.2 * min(ch_score / 1000, 1) +  # Normalized CH score\n",
    "                    0.2 * max(0, 1 - db_score / 5) +  # Inverted DB score\n",
    "                    0.15 * stability +\n",
    "                    0.15 * actionability_score\n",
    "                )\n",
    "            else:\n",
    "                overall_score = 0\n",
    "            \n",
    "            evaluation_results.append({\n",
    "                'Method': method,\n",
    "                'Clusters': n_clusters,\n",
    "                'Silhouette': f\"{sil_score:.3f}\" if sil_score > 0 else \"N/A\",\n",
    "                'CH Score': f\"{ch_score:.1f}\" if ch_score > 0 else \"N/A\",\n",
    "                'DB Score': f\"{db_score:.3f}\" if db_score != float('inf') else \"N/A\",\n",
    "                'Stability': f\"{stability:.3f}\",\n",
    "                'Min Size %': f\"{min_cluster_size_pct:.1f}%\",\n",
    "                'Actionability': f\"{actionability_score:.3f}\",\n",
    "                'Overall Score': f\"{overall_score:.3f}\",\n",
    "                'Noise Points': result.get('noise_points', 0)\n",
    "            })\n",
    "        \n",
    "        eval_df = pd.DataFrame(evaluation_results)\n",
    "        print(eval_df.to_string(index=False))\n",
    "        \n",
    "        # Business validation checks\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(\"BUSINESS VALIDATION CHECKS\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        for method, result in self.cluster_results.items():\n",
    "            labels = result['labels']\n",
    "            cluster_sizes = np.bincount(labels[labels >= 0])\n",
    "            \n",
    "            print(f\"\\n{method}:\")\n",
    "            print(f\"  ‚úì Number of clusters: {result['n_clusters']}\")\n",
    "            \n",
    "            # Size validation\n",
    "            min_size_pct = (np.min(cluster_sizes) / len(labels)) * 100 if len(cluster_sizes) > 0 else 0\n",
    "            if min_size_pct >= 5:\n",
    "                print(f\"  ‚úì All clusters >5% of population (min: {min_size_pct:.1f}%)\")\n",
    "            else:\n",
    "                print(f\"  ‚úó Some clusters <5% of population (min: {min_size_pct:.1f}%)\")\n",
    "            \n",
    "            # Stability validation\n",
    "            stability = result.get('stability_score', 0)\n",
    "            if stability >= 0.3:\n",
    "                print(f\"  ‚úì Good stability (score: {stability:.3f})\")\n",
    "            else:\n",
    "                print(f\"  ‚úó Low stability (score: {stability:.3f})\")\n",
    "        \n",
    "        # Recommend best method\n",
    "        if evaluation_results:\n",
    "            best_method_row = max(evaluation_results, key=lambda x: float(x['Overall Score']))\n",
    "            best_method = best_method_row['Method']\n",
    "            print(f\"\\nüèÜ RECOMMENDED METHOD: {best_method}\")\n",
    "            print(f\"   Overall Score: {best_method_row['Overall Score']}\")\n",
    "            print(f\"   Clusters: {best_method_row['Clusters']}\")\n",
    "            print(f\"   Silhouette: {best_method_row['Silhouette']}\")\n",
    "            \n",
    "        return eval_df\n",
    "    \n",
    "    def calculate_actionability_score(self, labels):\n",
    "        \"\"\"\n",
    "        Calculate how actionable/interpretable the clusters are\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Use original features for interpretability\n",
    "            data_to_use = self.processed_data_original if hasattr(self, 'processed_data_original') else self.processed_data\n",
    "            \n",
    "            if len(np.unique(labels)) <= 1:\n",
    "                return 0\n",
    "            \n",
    "            # Train a decision tree to see how well features separate clusters\n",
    "            mask = labels >= 0  # Exclude noise points\n",
    "            if np.sum(mask) == 0:\n",
    "                return 0\n",
    "                \n",
    "            tree = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "            tree.fit(data_to_use[mask], labels[mask])\n",
    "            \n",
    "            # Feature importance as proxy for actionability\n",
    "            feature_importance = np.mean(tree.feature_importances_)\n",
    "            \n",
    "            # Accuracy as separation quality\n",
    "            accuracy = tree.score(data_to_use[mask], labels[mask])\n",
    "            \n",
    "            return (feature_importance + accuracy) / 2\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def create_advanced_tsne_visualization(self, method='HDBSCAN', perplexity_values=[30, 50], plot_3d=False):\n",
    "        \"\"\"\n",
    "        Create advanced t-SNE visualization with multiple perplexity values\n",
    "        \"\"\"\n",
    "        if method not in self.cluster_results:\n",
    "            print(f\"Method {method} not found in results\")\n",
    "            return\n",
    "        \n",
    "        labels = self.cluster_results[method]['labels']\n",
    "        n_perplexity = len(perplexity_values)\n",
    "        \n",
    "        if plot_3d:\n",
    "            from mpl_toolkits.mplot3d import Axes3D\n",
    "            \n",
    "            fig = plt.figure(figsize=(15, 5 * n_perplexity))\n",
    "            \n",
    "            for i, perplexity in enumerate(perplexity_values):\n",
    "                print(f\"Creating 3D t-SNE with perplexity={perplexity}...\")\n",
    "                \n",
    "                tsne = TSNE(n_components=3, random_state=42, perplexity=perplexity)\n",
    "                tsne_data = tsne.fit_transform(self.processed_data)\n",
    "                \n",
    "                ax = fig.add_subplot(n_perplexity, 1, i+1, projection='3d')\n",
    "                scatter = ax.scatter(tsne_data[:, 0], tsne_data[:, 1], tsne_data[:, 2],\n",
    "                                  c=labels, cmap='viridis', alpha=0.7, s=50)\n",
    "                ax.set_title(f'3D t-SNE - {method} (perplexity={perplexity})')\n",
    "                ax.set_xlabel('t-SNE 1')\n",
    "                ax.set_ylabel('t-SNE 2')\n",
    "                ax.set_zlabel('t-SNE 3')\n",
    "                plt.colorbar(scatter, ax=ax, shrink=0.5)\n",
    "        else:\n",
    "            fig, axes = plt.subplots(1, n_perplexity, figsize=(7 * n_perplexity, 6))\n",
    "            if n_perplexity == 1:\n",
    "                axes = [axes]\n",
    "            \n",
    "            for i, perplexity in enumerate(perplexity_values):\n",
    "                print(f\"Creating 2D t-SNE with perplexity={perplexity}...\")\n",
    "                \n",
    "                tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n",
    "                tsne_data = tsne.fit_transform(self.processed_data)\n",
    "                \n",
    "                ax = axes[i]\n",
    "                scatter = ax.scatter(tsne_data[:, 0], tsne_data[:, 1], \n",
    "                                   c=labels, cmap='viridis', alpha=0.7, s=50)\n",
    "                ax.set_title(f't-SNE - {method} (perplexity={perplexity})')\n",
    "                ax.set_xlabel('t-SNE Component 1')\n",
    "                ax.set_ylabel('t-SNE Component 2')\n",
    "                \n",
    "                # Add cluster centers\n",
    "                unique_labels = np.unique(labels[labels >= 0])  # Exclude noise\n",
    "                for label in unique_labels:\n",
    "                    mask = labels == label\n",
    "                    if np.sum(mask) > 0:\n",
    "                        center_x = np.mean(tsne_data[mask, 0])\n",
    "                        center_y = np.mean(tsne_data[mask, 1])\n",
    "                        ax.annotate(f'C{label}', (center_x, center_y), \n",
    "                                  fontsize=12, fontweight='bold',\n",
    "                                  bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "                \n",
    "                plt.colorbar(scatter, ax=ax)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def create_feature_contribution_plot(self, method='HDBSCAN'):\n",
    "        \"\"\"\n",
    "        Create feature contribution plots showing which features drive clustering\n",
    "        \"\"\"\n",
    "        if method not in self.cluster_results:\n",
    "            print(f\"Method {method} not found in results\")\n",
    "            return\n",
    "        \n",
    "        labels = self.cluster_results[method]['labels']\n",
    "        \n",
    "        # Use original features for interpretability\n",
    "        if hasattr(self, 'processed_data_original'):\n",
    "            data_to_use = self.processed_data_original\n",
    "            feature_names = self.feature_names\n",
    "        else:\n",
    "            data_to_use = self.processed_data\n",
    "            feature_names = getattr(self, 'pca_feature_names', [f'Feature_{i}' for i in range(data_to_use.shape[1])])\n",
    "        \n",
    "        # Train a decision tree to understand feature importance\n",
    "        mask = labels >= 0  # Exclude noise points\n",
    "        if np.sum(mask) == 0:\n",
    "            print(\"No valid clusters to analyze\")\n",
    "            return\n",
    "        \n",
    "        tree = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "        tree.fit(data_to_use[mask], labels[mask])\n",
    "        \n",
    "        # Plot feature importance\n",
    "        importances = tree.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Feature importance bar plot\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.bar(range(min(10, len(importances))), importances[indices[:10]])\n",
    "        plt.title('Top 10 Feature Importances for Clustering')\n",
    "        plt.xlabel('Features')\n",
    "        plt.ylabel('Importance')\n",
    "        feature_labels = [feature_names[i] if i < len(feature_names) else f'Feature_{i}' for i in indices[:10]]\n",
    "        plt.xticks(range(min(10, len(importances))), feature_labels, rotation=45, ha='right')\n",
    "        \n",
    "        # Decision tree visualization (simplified)\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plot_tree(tree, max_depth=2, feature_names=feature_names[:len(feature_names)], \n",
    "                 class_names=[f'Cluster {i}' for i in np.unique(labels[mask])],\n",
    "                 filled=True, rounded=True, fontsize=8)\n",
    "        plt.title('Decision Tree (Depth 2)')\n",
    "        \n",
    "        # Cluster size distribution\n",
    "        plt.subplot(2, 2, 3)\n",
    "        cluster_counts = np.bincount(labels[labels >= 0])\n",
    "        cluster_ids = np.arange(len(cluster_counts))\n",
    "        plt.bar(cluster_ids, cluster_counts)\n",
    "        plt.title('Cluster Size Distribution')\n",
    "        plt.xlabel('Cluster ID')\n",
    "        plt.ylabel('Number of Points')\n",
    "        \n",
    "        # Feature correlation with clusters\n",
    "        plt.subplot(2, 2, 4)\n",
    "        if len(feature_names) > 0:\n",
    "            # Calculate correlation between first few features and cluster assignments\n",
    "            correlations = []\n",
    "            for i in range(min(5, data_to_use.shape[1])):\n",
    "                corr = np.corrcoef(data_to_use[mask, i], labels[mask])[0, 1]\n",
    "                correlations.append(abs(corr))\n",
    "            \n",
    "            plt.bar(range(len(correlations)), correlations)\n",
    "            plt.title('Feature-Cluster Correlations')\n",
    "            plt.xlabel('Features')\n",
    "            plt.ylabel('|Correlation| with Clusters')\n",
    "            plt.xticks(range(len(correlations)), \n",
    "                      [feature_names[i][:10] if i < len(feature_names) else f'F{i}' for i in range(len(correlations))], \n",
    "                      rotation=45, ha='right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def create_business_dashboard(self, method='HDBSCAN'):\n",
    "        \"\"\"\n",
    "        Create a business-focused dashboard showing cluster characteristics\n",
    "        \"\"\"\n",
    "        if method not in self.cluster_results:\n",
    "            print(f\"Method {method} not found in results\")\n",
    "            return\n",
    "        \n",
    "        labels = self.cluster_results[method]['labels']\n",
    "        df_with_clusters = self.df.copy()\n",
    "        df_with_clusters['Cluster'] = labels\n",
    "        \n",
    "        # Filter out noise points\n",
    "        df_filtered = df_with_clusters[df_with_clusters['Cluster'] >= 0]\n",
    "        \n",
    "        if len(df_filtered) == 0:\n",
    "            print(\"No valid clusters to display\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        # 1. Cluster size and stability\n",
    "        ax = axes[0]\n",
    "        cluster_counts = df_filtered['Cluster'].value_counts().sort_index()\n",
    "        bars = ax.bar(cluster_counts.index, cluster_counts.values)\n",
    "        ax.set_title('Cluster Sizes', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Cluster ID')\n",
    "        ax.set_ylabel('Number of Customers')\n",
    "        \n",
    "        # Add percentage labels\n",
    "        total = len(df_filtered)\n",
    "        for i, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + total*0.01,\n",
    "                   f'{height/total*100:.1f}%', ha='center', va='bottom')\n",
    "        \n",
    "        # 2. Age distribution by cluster\n",
    "        if 'age' in df_filtered.columns:\n",
    "            ax = axes[1]\n",
    "            df_filtered.boxplot(column='age', by='Cluster', ax=ax)\n",
    "            ax.set_title('Age Distribution by Cluster')\n",
    "            ax.set_xlabel('Cluster')\n",
    "            ax.set_ylabel('Age')\n",
    "        \n",
    "        # 3. Spending patterns\n",
    "        spending_col = None\n",
    "        if 'annual_spending' in df_filtered.columns:\n",
    "            spending_col = 'annual_spending'\n",
    "        elif 'spending_score' in df_filtered.columns:\n",
    "            spending_col = 'spending_score'\n",
    "        \n",
    "        if spending_col:\n",
    "            ax = axes[2]\n",
    "            if spending_col == 'spending_score':\n",
    "                # Categorical spending\n",
    "                spending_crosstab = pd.crosstab(df_filtered['Cluster'], df_filtered[spending_col], normalize='index')\n",
    "                spending_crosstab.plot(kind='bar', stacked=True, ax=ax)\n",
    "                ax.set_title('Spending Score Distribution by Cluster')\n",
    "                ax.set_ylabel('Proportion')\n",
    "            else:\n",
    "                # Numerical spending\n",
    "                df_filtered.boxplot(column=spending_col, by='Cluster', ax=ax)\n",
    "                ax.set_title('Spending Distribution by Cluster')\n",
    "                ax.set_ylabel('Annual Spending')\n",
    "            ax.set_xlabel('Cluster')\n",
    "            plt.setp(ax.xaxis.get_majorticklabels(), rotation=0)\n",
    "        \n",
    "        # 4. Education levels\n",
    "        if 'education' in df_filtered.columns:\n",
    "            ax = axes[3]\n",
    "            edu_crosstab = pd.crosstab(df_filtered['Cluster'], df_filtered['education'], normalize='index')\n",
    "            edu_crosstab.plot(kind='bar', stacked=True, ax=ax)\n",
    "            ax.set_title('Education Distribution by Cluster')\n",
    "            ax.set_xlabel('Cluster')\n",
    "            ax.set_ylabel('Proportion')\n",
    "            plt.setp(ax.xaxis.get_majorticklabels(), rotation=0)\n",
    "        \n",
    "        # 5. Life stage distribution\n",
    "        if 'life_stage' in df_filtered.columns:\n",
    "            ax = axes[4]\n",
    "            life_crosstab = pd.crosstab(df_filtered['Cluster'], df_filtered['life_stage'], normalize='index')\n",
    "            life_crosstab.plot(kind='bar', stacked=True, ax=ax)\n",
    "            ax.set_title('Life Stage Distribution by Cluster')\n",
    "            ax.set_xlabel('Cluster')\n",
    "            ax.set_ylabel('Proportion')\n",
    "            plt.setp(ax.xaxis.get_majorticklabels(), rotation=0)\n",
    "        \n",
    "        # 6. Composite scores\n",
    "        if 'economic_capacity_index' in df_filtered.columns:\n",
    "            ax = axes[5]\n",
    "            df_filtered.boxplot(column='economic_capacity_index', by='Cluster', ax=ax)\n",
    "            ax.set_title('Economic Capacity by Cluster')\n",
    "            ax.set_xlabel('Cluster')\n",
    "            ax.set_ylabel('Economic Capacity Index')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print cluster summaries\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"BUSINESS CLUSTER SUMMARIES - {method}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        for cluster_id in sorted(df_filtered['Cluster'].unique()):\n",
    "            cluster_data = df_filtered[df_filtered['Cluster'] == cluster_id]\n",
    "            pct = len(cluster_data) / len(df_filtered) * 100\n",
    "            \n",
    "            print(f\"\\nüéØ CLUSTER {cluster_id} ({len(cluster_data)} customers, {pct:.1f}%)\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Demographics\n",
    "            if 'age' in cluster_data.columns:\n",
    "                print(f\"Age: {cluster_data['age'].mean():.1f} ¬± {cluster_data['age'].std():.1f} years\")\n",
    "            \n",
    "            # Life stage\n",
    "            if 'life_stage' in cluster_data.columns:\n",
    "                top_life_stage = cluster_data['life_stage'].mode().iloc[0] if not cluster_data['life_stage'].mode().empty else 'Unknown'\n",
    "                print(f\"Primary Life Stage: {top_life_stage}\")\n",
    "            \n",
    "            # Economic indicators\n",
    "            if 'economic_capacity_index' in cluster_data.columns:\n",
    "                print(f\"Economic Capacity: {cluster_data['economic_capacity_index'].mean():.1f}/100\")\n",
    "            \n",
    "            # Spending\n",
    "            if spending_col:\n",
    "                if spending_col == 'spending_score':\n",
    "                    top_spending = cluster_data[spending_col].mode().iloc[0] if not cluster_data[spending_col].mode().empty else 'Unknown'\n",
    "                    print(f\"Spending Level: {top_spending}\")\n",
    "                else:\n",
    "                    print(f\"Average Spending: ${cluster_data[spending_col].mean():.0f}\")\n",
    "            \n",
    "            # Professional category\n",
    "            if 'profession_category' in cluster_data.columns:\n",
    "                top_profession = cluster_data['profession_category'].mode().iloc[0] if not cluster_data['profession_category'].mode().empty else 'Unknown'\n",
    "                print(f\"Primary Profession Type: {top_profession}\")\n",
    "    \n",
    "    def generate_segment_names(self, method='HDBSCAN'):\n",
    "        \"\"\"\n",
    "        Generate descriptive names for clusters based on their characteristics\n",
    "        \"\"\"\n",
    "        if method not in self.cluster_results:\n",
    "            print(f\"Method {method} not found in results\")\n",
    "            return\n",
    "        \n",
    "        labels = self.cluster_results[method]['labels']\n",
    "        df_with_clusters = self.df.copy()\n",
    "        df_with_clusters['Cluster'] = labels\n",
    "        \n",
    "        # Filter out noise points\n",
    "        df_filtered = df_with_clusters[df_with_clusters['Cluster'] >= 0]\n",
    "        \n",
    "        segment_names = {}\n",
    "        \n",
    "        for cluster_id in sorted(df_filtered['Cluster'].unique()):\n",
    "            cluster_data = df_filtered[df_filtered['Cluster'] == cluster_id]\n",
    "            \n",
    "            # Analyze key characteristics\n",
    "            descriptors = []\n",
    "            \n",
    "            # Age-based descriptor\n",
    "            if 'age' in cluster_data.columns:\n",
    "                avg_age = cluster_data['age'].mean()\n",
    "                if avg_age < 30:\n",
    "                    descriptors.append('Young')\n",
    "                elif avg_age > 50:\n",
    "                    descriptors.append('Senior')\n",
    "                else:\n",
    "                    descriptors.append('Mid-Career')\n",
    "            \n",
    "            # Profession-based descriptor\n",
    "            if 'profession_category' in cluster_data.columns:\n",
    "                top_profession = cluster_data['profession_category'].mode().iloc[0] if not cluster_data['profession_category'].mode().empty else None\n",
    "                if top_profession:\n",
    "                    if top_profession == 'High-Skill':\n",
    "                        descriptors.append('Professionals')\n",
    "                    elif top_profession == 'Creative':\n",
    "                        descriptors.append('Creatives')\n",
    "                    elif top_profession == 'Service/Care':\n",
    "                        descriptors.append('Service Workers')\n",
    "                    elif top_profession == 'Business-Oriented':\n",
    "                        descriptors.append('Business')\n",
    "            \n",
    "            # Spending-based descriptor\n",
    "            spending_col = 'annual_spending' if 'annual_spending' in cluster_data.columns else 'spending_score'\n",
    "            if spending_col in cluster_data.columns:\n",
    "                if spending_col == 'spending_score':\n",
    "                    top_spending = cluster_data[spending_col].mode().iloc[0] if not cluster_data[spending_col].mode().empty else None\n",
    "                    if top_spending == 'High':\n",
    "                        descriptors.append('High Spenders')\n",
    "                    elif top_spending == 'Low':\n",
    "                        descriptors.append('Budget Conscious')\n",
    "                else:\n",
    "                    avg_spending = cluster_data[spending_col].mean()\n",
    "                    overall_avg = df_filtered[spending_col].mean()\n",
    "                    if avg_spending > overall_avg * 1.2:\n",
    "                        descriptors.append('High Spenders')\n",
    "                    elif avg_spending < overall_avg * 0.8:\n",
    "                        descriptors.append('Budget Conscious')\n",
    "            \n",
    "            # Generate name\n",
    "            if len(descriptors) >= 2:\n",
    "                name = ' '.join(descriptors[:2])\n",
    "            elif len(descriptors) == 1:\n",
    "                name = descriptors[0]\n",
    "            else:\n",
    "                name = f'Segment {cluster_id}'\n",
    "            \n",
    "            segment_names[cluster_id] = name\n",
    "        \n",
    "        print(f\"\\nüè∑Ô∏è  SUGGESTED SEGMENT NAMES - {method}\")\n",
    "        print(\"-\" * 50)\n",
    "        for cluster_id, name in segment_names.items():\n",
    "            cluster_size = len(df_filtered[df_filtered['Cluster'] == cluster_id])\n",
    "            pct = cluster_size / len(df_filtered) * 100\n",
    "            print(f\"Cluster {cluster_id}: '{name}' ({cluster_size} customers, {pct:.1f}%)\")\n",
    "        \n",
    "        return segment_names\n",
    "        \"\"\"\n",
    "        Create t-SNE visualization of clusters\n",
    "        \"\"\"\n",
    "        if method not in self.cluster_results:\n",
    "            print(f\"Method {method} not found in results\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Creating t-SNE visualization for {method}...\")\n",
    "        \n",
    "        # Apply t-SNE\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n",
    "        tsne_data = tsne.fit_transform(self.processed_data)\n",
    "        \n",
    "        labels = self.cluster_results[method]['labels']\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        scatter = plt.scatter(tsne_data[:, 0], tsne_data[:, 1], \n",
    "                            c=labels, cmap='viridis', alpha=0.7, s=50)\n",
    "        plt.colorbar(scatter)\n",
    "        plt.title(f't-SNE Visualization - {method}')\n",
    "        plt.xlabel('t-SNE Component 1')\n",
    "        plt.ylabel('t-SNE Component 2')\n",
    "        \n",
    "        # Add cluster centers if available\n",
    "        unique_labels = np.unique(labels)\n",
    "        for label in unique_labels:\n",
    "            mask = labels == label\n",
    "            center_x = np.mean(tsne_data[mask, 0])\n",
    "            center_y = np.mean(tsne_data[mask, 1])\n",
    "            plt.annotate(f'C{label}', (center_x, center_y), \n",
    "                        fontsize=12, fontweight='bold',\n",
    "                        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def create_cluster_profiles(self, method='K-Means'):\n",
    "        \"\"\"\n",
    "        Create detailed cluster profiles showing characteristics\n",
    "        \"\"\"\n",
    "        if method not in self.cluster_results:\n",
    "            print(f\"Method {method} not found in results\")\n",
    "            return\n",
    "        \n",
    "        labels = self.cluster_results[method]['labels']\n",
    "        df_with_clusters = self.df.copy()\n",
    "        df_with_clusters['Cluster'] = labels\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"CLUSTER PROFILES - {method}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Numerical features analysis\n",
    "        numerical_cols = self.preprocessing_info['numerical_cols']\n",
    "        if numerical_cols:\n",
    "            print(\"\\nNumerical Features by Cluster:\")\n",
    "            numerical_stats = df_with_clusters.groupby('Cluster')[numerical_cols].agg(['mean', 'std'])\n",
    "            print(numerical_stats.round(2))\n",
    "        \n",
    "        # Categorical features analysis\n",
    "        categorical_cols = self.preprocessing_info['categorical_cols']\n",
    "        if categorical_cols:\n",
    "            print(\"\\nCategorical Features Distribution by Cluster:\")\n",
    "            for col in categorical_cols:\n",
    "                print(f\"\\n{col.upper()}:\")\n",
    "                crosstab = pd.crosstab(df_with_clusters['Cluster'], \n",
    "                                     df_with_clusters[col], normalize='index')\n",
    "                print((crosstab * 100).round(1).to_string())\n",
    "        \n",
    "        return df_with_clusters\n",
    "    \n",
    "    def create_radial_charts(self, method='K-Means', sample_features=None):\n",
    "        \"\"\"\n",
    "        Create radial/radar charts for cluster comparison\n",
    "        \"\"\"\n",
    "        if method not in self.cluster_results:\n",
    "            print(f\"Method {method} not found in results\")\n",
    "            return\n",
    "        \n",
    "        labels = self.cluster_results[method]['labels']\n",
    "        df_with_clusters = self.df.copy()\n",
    "        df_with_clusters['Cluster'] = labels\n",
    "        \n",
    "        # Select features for radial chart\n",
    "        numerical_cols = self.preprocessing_info['numerical_cols']\n",
    "        if sample_features is None:\n",
    "            sample_features = numerical_cols[:6]  # Limit to 6 features for readability\n",
    "        \n",
    "        if not sample_features:\n",
    "            print(\"No numerical features available for radial chart\")\n",
    "            return\n",
    "        \n",
    "        # Calculate means for each cluster\n",
    "        cluster_means = df_with_clusters.groupby('Cluster')[sample_features].mean()\n",
    "        \n",
    "        # Normalize values to 0-1 scale for better visualization\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        scaler = MinMaxScaler()\n",
    "        normalized_means = pd.DataFrame(\n",
    "            scaler.fit_transform(cluster_means),\n",
    "            columns=cluster_means.columns,\n",
    "            index=cluster_means.index\n",
    "        )\n",
    "        \n",
    "        # Create radial charts with automatic grid layout\n",
    "        n_clusters = len(normalized_means)\n",
    "        \n",
    "        # Calculate optimal grid dimensions\n",
    "        if n_clusters <= 3:\n",
    "            n_rows, n_cols = 1, n_clusters\n",
    "        elif n_clusters <= 6:\n",
    "            n_rows, n_cols = 2, (n_clusters + 1) // 2\n",
    "        elif n_clusters <= 9:\n",
    "            n_rows, n_cols = 3, (n_clusters + 2) // 3\n",
    "        else:\n",
    "            n_rows = int(np.ceil(np.sqrt(n_clusters)))\n",
    "            n_cols = int(np.ceil(n_clusters / n_rows))\n",
    "        \n",
    "        # Adjust figure size based on grid\n",
    "        fig_width = min(5 * n_cols, 20)  # Cap width at 20\n",
    "        fig_height = min(5 * n_rows, 15)  # Cap height at 15\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(fig_width, fig_height), \n",
    "                               subplot_kw=dict(projection='polar'))\n",
    "        \n",
    "        # Handle different cases for axes array structure\n",
    "        if n_clusters == 1:\n",
    "            axes = [axes]\n",
    "        elif n_rows == 1:\n",
    "            axes = axes if n_cols > 1 else [axes]\n",
    "        else:\n",
    "            axes = axes.flatten()\n",
    "        \n",
    "        angles = np.linspace(0, 2*np.pi, len(sample_features), endpoint=False).tolist()\n",
    "        angles += angles[:1]  # Complete the circle\n",
    "        \n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, n_clusters))\n",
    "        \n",
    "        for idx, (cluster_id, cluster_data) in enumerate(normalized_means.iterrows()):\n",
    "            values = cluster_data.tolist()\n",
    "            values += values[:1]  # Complete the circle\n",
    "            \n",
    "            ax = axes[idx]\n",
    "            ax.plot(angles, values, 'o-', linewidth=2, color=colors[idx])\n",
    "            ax.fill(angles, values, alpha=0.25, color=colors[idx])\n",
    "            ax.set_xticks(angles[:-1])\n",
    "            ax.set_xticklabels(sample_features)\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.set_title(f'Cluster {cluster_id}', size=14, weight='bold', pad=20)\n",
    "            ax.grid(True)\n",
    "        \n",
    "        # Hide unused subplots if any\n",
    "        total_subplots = n_rows * n_cols\n",
    "        if n_clusters < total_subplots:\n",
    "            for i in range(n_clusters, total_subplots):\n",
    "                axes[i].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def create_comparison_heatmap(self, method='K-Means'):\n",
    "        \"\"\"\n",
    "        Create heatmap showing cluster characteristics\n",
    "        \"\"\"\n",
    "        if method not in self.cluster_results:\n",
    "            print(f\"Method {method} not found in results\")\n",
    "            return\n",
    "        \n",
    "        labels = self.cluster_results[method]['labels']\n",
    "        df_with_clusters = self.df.copy()\n",
    "        df_with_clusters['Cluster'] = labels\n",
    "        \n",
    "        # Get numerical features means by cluster\n",
    "        numerical_cols = self.preprocessing_info['numerical_cols']\n",
    "        if not numerical_cols:\n",
    "            print(\"No numerical features for heatmap\")\n",
    "            return\n",
    "        \n",
    "        cluster_means = df_with_clusters.groupby('Cluster')[numerical_cols].mean()\n",
    "        \n",
    "        # Normalize for better visualization\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        normalized_data = scaler.fit_transform(cluster_means.T).T\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.heatmap(normalized_data, \n",
    "                   xticklabels=numerical_cols,\n",
    "                   yticklabels=[f'Cluster {i}' for i in cluster_means.index],\n",
    "                   annot=True, fmt='.2f', cmap='RdYlBu_r', center=0)\n",
    "        plt.title(f'Cluster Characteristics Heatmap - {method}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def create_enhanced_radial_charts(self, method='HDBSCAN', sample_features=None, show_comparison=True):\n",
    "        \"\"\"\n",
    "        Create enhanced radial charts with feature importance weighting and comparison mode\n",
    "        \"\"\"\n",
    "        if method not in self.cluster_results:\n",
    "            print(f\"Method {method} not found in results\")\n",
    "            return\n",
    "        \n",
    "        labels = self.cluster_results[method]['labels']\n",
    "        df_with_clusters = self.df.copy()\n",
    "        df_with_clusters['Cluster'] = labels\n",
    "        \n",
    "        # Filter out noise points\n",
    "        df_filtered = df_with_clusters[df_with_clusters['Cluster'] >= 0]\n",
    "        \n",
    "        # Select features for radial chart - prioritize composite features\n",
    "        composite_features = ['career_development_score', 'economic_capacity_index', 'family_responsibility_factor']\n",
    "        numerical_cols = [col for col in self.preprocessing_info.get('numerical_cols', []) if col in df_filtered.columns]\n",
    "        \n",
    "        if sample_features is None:\n",
    "            # Prioritize composite features, then numerical\n",
    "            available_composite = [f for f in composite_features if f in df_filtered.columns]\n",
    "            available_numerical = [f for f in numerical_cols if f not in available_composite]\n",
    "            sample_features = available_composite + available_numerical[:6-len(available_composite)]\n",
    "            sample_features = sample_features[:6]  # Limit to 6 for readability\n",
    "        \n",
    "        if not sample_features:\n",
    "            print(\"No suitable features available for radial chart\")\n",
    "            return\n",
    "        \n",
    "        # Calculate means for each cluster\n",
    "        cluster_means = df_filtered.groupby('Cluster')[sample_features].mean()\n",
    "        \n",
    "        # Normalize values to 0-1 scale\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        scaler = MinMaxScaler()\n",
    "        normalized_means = pd.DataFrame(\n",
    "            scaler.fit_transform(cluster_means),\n",
    "            columns=cluster_means.columns,\n",
    "            index=cluster_means.index\n",
    "        )\n",
    "        \n",
    "        if show_comparison:\n",
    "            # Single plot comparing all clusters\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "            \n",
    "            angles = np.linspace(0, 2*np.pi, len(sample_features), endpoint=False).tolist()\n",
    "            angles += angles[:1]  # Complete the circle\n",
    "            \n",
    "            colors = plt.cm.Set3(np.linspace(0, 1, len(normalized_means)))\n",
    "            \n",
    "            for idx, (cluster_id, cluster_data) in enumerate(normalized_means.iterrows()):\n",
    "                values = cluster_data.tolist()\n",
    "                values += values[:1]  # Complete the circle\n",
    "                \n",
    "                ax.plot(angles, values, 'o-', linewidth=3, color=colors[idx], \n",
    "                       label=f'Cluster {cluster_id}', alpha=0.8)\n",
    "                ax.fill(angles, values, alpha=0.15, color=colors[idx])\n",
    "            \n",
    "            ax.set_xticks(angles[:-1])\n",
    "            ax.set_xticklabels([f.replace('_', ' ').title() for f in sample_features])\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.set_title('Cluster Comparison - All Segments', size=16, weight='bold', pad=30)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Individual cluster charts with automatic grid layout\n",
    "        n_clusters = len(normalized_means)\n",
    "        \n",
    "        # Calculate optimal grid dimensions\n",
    "        if n_clusters <= 3:\n",
    "            n_rows, n_cols = 1, n_clusters\n",
    "        elif n_clusters <= 6:\n",
    "            n_rows, n_cols = 2, (n_clusters + 1) // 2\n",
    "        elif n_clusters <= 9:\n",
    "            n_rows, n_cols = 3, (n_clusters + 2) // 3\n",
    "        else:\n",
    "            n_rows = int(np.ceil(np.sqrt(n_clusters)))\n",
    "            n_cols = int(np.ceil(n_clusters / n_rows))\n",
    "        \n",
    "        # Adjust figure size based on grid\n",
    "        fig_width = min(5 * n_cols, 20)\n",
    "        fig_height = min(5 * n_rows, 15)\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(fig_width, fig_height), \n",
    "                               subplot_kw=dict(projection='polar'))\n",
    "        \n",
    "        # Handle different cases for axes array structure\n",
    "        if n_clusters == 1:\n",
    "            axes = [axes]\n",
    "        elif n_rows == 1:\n",
    "            axes = axes if n_cols > 1 else [axes]\n",
    "        else:\n",
    "            axes = axes.flatten()\n",
    "        \n",
    "        angles = np.linspace(0, 2*np.pi, len(sample_features), endpoint=False).tolist()\n",
    "        angles += angles[:1]  # Complete the circle\n",
    "        \n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, n_clusters))\n",
    "        \n",
    "        for idx, (cluster_id, cluster_data) in enumerate(normalized_means.iterrows()):\n",
    "            values = cluster_data.tolist()\n",
    "            values += values[:1]  # Complete the circle\n",
    "            \n",
    "            ax = axes[idx]\n",
    "            ax.plot(angles, values, 'o-', linewidth=3, color=colors[idx])\n",
    "            ax.fill(angles, values, alpha=0.25, color=colors[idx])\n",
    "            ax.set_xticks(angles[:-1])\n",
    "            ax.set_xticklabels([f.replace('_', ' ').title()[:15] for f in sample_features], fontsize=10)\n",
    "            ax.set_ylim(0, 1)\n",
    "            \n",
    "            # Add cluster size info\n",
    "            cluster_size = len(df_filtered[df_filtered['Cluster'] == cluster_id])\n",
    "            cluster_pct = cluster_size / len(df_filtered) * 100\n",
    "            ax.set_title(f'Cluster {cluster_id}\\n({cluster_size} customers, {cluster_pct:.1f}%)', \n",
    "                        size=12, weight='bold', pad=20)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Hide unused subplots if any\n",
    "        total_subplots = n_rows * n_cols\n",
    "        if n_clusters < total_subplots:\n",
    "            for i in range(n_clusters, total_subplots):\n",
    "                axes[i].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def create_cluster_stability_analysis(self, methods=['HDBSCAN', 'K-Means'], n_iterations=20):\n",
    "        \"\"\"\n",
    "        Analyze cluster stability across multiple runs\n",
    "        \"\"\"\n",
    "        print(\"Performing cluster stability analysis...\")\n",
    "        \n",
    "        stability_results = {}\n",
    "        \n",
    "        for method in methods:\n",
    "            if method not in self.cluster_results:\n",
    "                continue\n",
    "                \n",
    "            print(f\"Analyzing stability for {method}...\")\n",
    "            \n",
    "            # Get the original clustering\n",
    "            original_labels = self.cluster_results[method]['labels']\n",
    "            n_clusters = self.cluster_results[method]['n_clusters']\n",
    "            \n",
    "            # Perform multiple clusterings with bootstrap sampling\n",
    "            stability_scores = []\n",
    "            \n",
    "            for i in range(n_iterations):\n",
    "                # Bootstrap sample\n",
    "                n_samples = len(self.processed_data)\n",
    "                indices = np.random.choice(n_samples, size=int(0.8 * n_samples), replace=True)\n",
    "                bootstrap_data = self.processed_data[indices]\n",
    "                \n",
    "                # Apply clustering\n",
    "                try:\n",
    "                    if method == 'HDBSCAN':\n",
    "                        clusterer = hdbscan.HDBSCAN(\n",
    "                            min_cluster_size=max(5, len(bootstrap_data) // 50),\n",
    "                            min_samples=3\n",
    "                        )\n",
    "                        bootstrap_labels = clusterer.fit_predict(bootstrap_data)\n",
    "                    elif method == 'K-Means':\n",
    "                        clusterer = KMeans(n_clusters=n_clusters, random_state=i, n_init=10)\n",
    "                        bootstrap_labels = clusterer.fit_predict(bootstrap_data)\n",
    "                    elif method == 'Gaussian Mixture':\n",
    "                        clusterer = GaussianMixture(n_components=n_clusters, random_state=i)\n",
    "                        clusterer.fit(bootstrap_data)\n",
    "                        bootstrap_labels = clusterer.predict(bootstrap_data)\n",
    "                    else:\n",
    "                        continue\n",
    "                    \n",
    "                    # Calculate adjusted rand index with original clustering (subset)\n",
    "                    original_subset = original_labels[indices]\n",
    "                    if len(np.unique(bootstrap_labels)) > 1 and len(np.unique(original_subset)) > 1:\n",
    "                        stability_score = adjusted_rand_score(original_subset, bootstrap_labels)\n",
    "                        stability_scores.append(stability_score)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "            \n",
    "            if stability_scores:\n",
    "                stability_results[method] = {\n",
    "                    'mean_stability': np.mean(stability_scores),\n",
    "                    'std_stability': np.std(stability_scores),\n",
    "                    'scores': stability_scores\n",
    "                }\n",
    "        \n",
    "        # Plot stability results\n",
    "        if stability_results:\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "            \n",
    "            # Bar plot of mean stability\n",
    "            methods = list(stability_results.keys())\n",
    "            means = [stability_results[m]['mean_stability'] for m in methods]\n",
    "            stds = [stability_results[m]['std_stability'] for m in methods]\n",
    "            \n",
    "            ax1.bar(methods, means, yerr=stds, capsize=5, alpha=0.7)\n",
    "            ax1.set_title('Cluster Stability Comparison')\n",
    "            ax1.set_ylabel('Mean Adjusted Rand Index')\n",
    "            ax1.set_ylim(0, 1)\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Distribution plot\n",
    "            for i, method in enumerate(methods):\n",
    "                scores = stability_results[method]['scores']\n",
    "                ax2.hist(scores, alpha=0.6, label=method, bins=10)\n",
    "            \n",
    "            ax2.set_title('Stability Score Distributions')\n",
    "            ax2.set_xlabel('Adjusted Rand Index')\n",
    "            ax2.set_ylabel('Frequency')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(\"STABILITY ANALYSIS RESULTS\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            for method, results in stability_results.items():\n",
    "                mean_stab = results['mean_stability']\n",
    "                std_stab = results['std_stability']\n",
    "                print(f\"{method}: {mean_stab:.3f} ¬± {std_stab:.3f}\")\n",
    "                \n",
    "                if mean_stab >= 0.7:\n",
    "                    print(f\"  ‚úì Highly stable clustering\")\n",
    "                elif mean_stab >= 0.5:\n",
    "                    print(f\"  ‚úì Moderately stable clustering\")\n",
    "                else:\n",
    "                    print(f\"  ‚ö† Low stability - consider parameter tuning\")\n",
    "        \n",
    "        return stability_results    \n",
    "        \n",
    "    def evaluate_clusters(self):\n",
    "        \"\"\"\n",
    "        Enhanced cluster evaluation with business validation\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"COMPREHENSIVE CLUSTERING EVALUATION\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        evaluation_results = []\n",
    "        \n",
    "        for method, result in self.cluster_results.items():\n",
    "            labels = result['labels']\n",
    "            n_clusters = result['n_clusters']\n",
    "            \n",
    "            # Basic metrics\n",
    "            sil_score = result.get('silhouette_score', -1)\n",
    "            ch_score = result.get('calinski_harabasz_score', 0)\n",
    "            db_score = result.get('davies_bouldin_score', float('inf'))\n",
    "            stability = result.get('stability_score', 0)\n",
    "            \n",
    "            # Business validation metrics\n",
    "            cluster_sizes = np.bincount(labels[labels >= 0])  # Exclude noise points\n",
    "            min_cluster_size_pct = (np.min(cluster_sizes) / len(labels)) * 100 if len(cluster_sizes) > 0 else 0\n",
    "            \n",
    "            # Actionability score (based on feature separation)\n",
    "            actionability_score = self.calculate_actionability_score(labels)\n",
    "            \n",
    "            # Overall score (weighted combination)\n",
    "            if sil_score > 0:\n",
    "                overall_score = (\n",
    "                    0.3 * sil_score +\n",
    "                    0.2 * min(ch_score / 1000, 1) +  # Normalized CH score\n",
    "                    0.2 * max(0, 1 - db_score / 5) +  # Inverted DB score\n",
    "                    0.15 * stability +\n",
    "                    0.15 * actionability_score\n",
    "                )\n",
    "            else:\n",
    "                overall_score = 0\n",
    "            \n",
    "            evaluation_results.append({\n",
    "                'Method': method,\n",
    "                'Clusters': n_clusters,\n",
    "                'Silhouette': f\"{sil_score:.3f}\" if sil_score > 0 else \"N/A\",\n",
    "                'CH Score': f\"{ch_score:.1f}\" if ch_score > 0 else \"N/A\",\n",
    "                'DB Score': f\"{db_score:.3f}\" if db_score != float('inf') else \"N/A\",\n",
    "                'Stability': f\"{stability:.3f}\",\n",
    "                'Min Size %': f\"{min_cluster_size_pct:.1f}%\",\n",
    "                'Actionability': f\"{actionability_score:.3f}\",\n",
    "                'Overall Score': f\"{overall_score:.3f}\",\n",
    "                'Noise Points': result.get('noise_points', 0)\n",
    "            })\n",
    "        \n",
    "        eval_df = pd.DataFrame(evaluation_results)\n",
    "        print(eval_df.to_string(index=False))\n",
    "        \n",
    "        # Business validation checks\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(\"BUSINESS VALIDATION CHECKS\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        for method, result in self.cluster_results.items():\n",
    "            labels = result['labels']\n",
    "            cluster_sizes = np.bincount(labels[labels >= 0])\n",
    "            \n",
    "            print(f\"\\n{method}:\")\n",
    "            print(f\"  ‚úì Number of clusters: {result['n_clusters']}\")\n",
    "            \n",
    "            # Size validation\n",
    "            min_size_pct = (np.min(cluster_sizes) / len(labels)) * 100 if len(cluster_sizes) > 0 else 0\n",
    "            if min_size_pct >= 5:\n",
    "                print(f\"  ‚úì All clusters >5% of population (min: {min_size_pct:.1f}%)\")\n",
    "            else:\n",
    "                print(f\"  ‚úó Some clusters <5% of population (min: {min_size_pct:.1f}%)\")\n",
    "            \n",
    "            # Stability validation\n",
    "            stability = result.get('stability_score', 0)\n",
    "            if stability >= 0.3:\n",
    "                print(f\"  ‚úì Good stability (score: {stability:.3f})\")\n",
    "            else:\n",
    "                print(f\"  ‚úó Low stability (score: {stability:.3f})\")\n",
    "        \n",
    "        # Recommend best method\n",
    "        valid_methods = {k: v for k, v in evaluation_results if v['Overall Score'] != '0.000'}\n",
    "        \n",
    "        if valid_methods:\n",
    "            best_method_row = max(evaluation_results, key=lambda x: float(x['Overall Score']))\n",
    "            best_method = best_method_row['Method']\n",
    "            print(f\"\\nüèÜ RECOMMENDED METHOD: {best_method}\")\n",
    "            print(f\"   Overall Score: {best_method_row['Overall Score']}\")\n",
    "            print(f\"   Clusters:        print(f\"Suggested optimal number of clusters: {optimal_k}\")\n",
    "        \n",
    "        # Also calculate Gap Statistic\n",
    "        gap_optimal_k, _ = self.gap_statistic(max_clusters=max_clusters)\n",
    "        \n",
    "        print(f\"Gap Statistic suggests: {gap_optimal_k}\")\n",
    "        print(f\"Final recommendation: {optimal_k} (based on silhouette score)\")\n",
    "        \n",
    "        return optimal_k        self.preprocessing_info = {}\n",
    "        \n",
    "    def preprocess_data(self, categorical_cols=None, numerical_cols=None, use_pca=True, pca_variance_threshold=0.95):\n",
    "        \"\"\"\n",
    "        Enhanced preprocessing with composite features and advanced encoding\n",
    "        \"\"\"\n",
    "        print(\"Starting enhanced data preprocessing...\")\n",
    "        \n",
    "        # Create composite features first\n",
    "        self.create_composite_features()\n",
    "        \n",
    "        # Auto-detect column types if not specified\n",
    "        if categorical_cols is None:\n",
    "            categorical_cols = self.df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        if numerical_cols is None:\n",
    "            numerical_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        # Identify ordinal features\n",
    "        ordinal_features = {}\n",
    "        if 'spending_score' in categorical_cols:\n",
    "            ordinal_features['spending_score'] = ['Low', 'Average', 'High']\n",
    "        if 'education' in categorical_cols:\n",
    "            ordinal_features['education'] = ['High School', 'Bachelor', 'Master', 'PhD']\n",
    "        if 'income_bracket' in categorical_cols:\n",
    "            ordinal_features['income_bracket'] = ['Low', 'Medium', 'High']\n",
    "        \n",
    "        # Separate ordinal from regular categorical\n",
    "        ordinal_cols = list(ordinal_features.keys())\n",
    "        regular_categorical = [col for col in categorical_cols if col not in ordinal_cols]\n",
    "        \n",
    "        self.preprocessing_info = {\n",
    "            'categorical_cols': regular_categorical,\n",
    "            'numerical_cols': numerical_cols,\n",
    "            'ordinal_cols': ordinal_cols,\n",
    "            'ordinal_mappings': ordinal_features\n",
    "        }\n",
    "        \n",
    "        # Create preprocessing transformers\n",
    "        transformers = []\n",
    "        \n",
    "        # Numerical features - use RobustScaler for outlier resistance\n",
    "        if numerical_cols:\n",
    "            transformers.append(('num', RobustScaler(), numerical_cols))\n",
    "        \n",
    "        # Ordinal features\n",
    "        for col, categories in ordinal_features.items():\n",
    "            if col in self.df.columns:\n",
    "                transformers.append((f'ord_{col}', OrdinalEncoder(categories=[categories]), [col]))\n",
    "        \n",
    "        # Regular categorical features - OneHot encoding\n",
    "        if regular_categorical:\n",
    "            # Filter out high cardinality features (>10 unique values)\n",
    "            low_card_categorical = []\n",
    "            for col in regular_categorical:\n",
    "                if col in self.df.columns and self.df[col].nunique() <= 10:\n",
    "                    low_card_categorical.append(col)\n",
    "            \n",
    "            if low_card_categorical:\n",
    "                transformers.append(('cat', OneHotEncoder(drop='first', sparse_output=False), low_card_categorical))\n",
    "        \n",
    "        # Apply preprocessing\n",
    "        if transformers:\n",
    "            preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')\n",
    "            processed_data = preprocessor.fit_transform(self.df)\n",
    "            self.preprocessor = preprocessor\n",
    "            \n",
    "            # Get feature names\n",
    "            feature_names = []\n",
    "            for name, transformer, cols in transformers:\n",
    "                if name == 'num':\n",
    "                    feature_names.extend(cols)\n",
    "                elif name.startswith('ord_'):\n",
    "                    feature_names.extend(cols)\n",
    "                elif name == 'cat':\n",
    "                    if hasattr(transformer, 'get_feature_names_out'):\n",
    "                        feature_names.extend(transformer.get_feature_names_out(cols))\n",
    "                    else:\n",
    "                        feature_names.extend(cols)\n",
    "            \n",
    "            self.feature_names = feature_names\n",
    "        else:\n",
    "            processed_data = self.df.select_dtypes(include=[np.number]).values\n",
    "            self.feature_names = self.df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        # Apply PCA if requested\n",
    "        if use_pca and processed_data.shape[1] > 5:\n",
    "            print(\"Applying PCA for dimensionality reduction...\")\n",
    "            self.pca = PCA()\n",
    "            pca_data = self.pca.fit_transform(processed_data)\n",
    "            \n",
    "            # Determine number of components based on variance threshold\n",
    "            cumsum_var = np.cumsum(self.pca.explained_variance_ratio_)\n",
    "            n_components = np.argmax(cumsum_var >= pca_variance_threshold) + 1\n",
    "            \n",
    "            print(f\"PCA: Using {n_components} components explaining {cumsum_var[n_components-1]:.1%} of variance\")\n",
    "            \n",
    "            # Refit with optimal components\n",
    "            self.pca = PCA(n_components=n_components)\n",
    "            self.processed_data = self.pca.fit_transform(processed_data)\n",
    "            self.use_pca = True\n",
    "            \n",
    "            # Store both versions\n",
    "            self.processed_data_original = processed_data\n",
    "            self.pca_feature_names = [f'PC{i+1}' for i in range(n_components)]\n",
    "        else:\n",
    "            self.processed_data = processed_data\n",
    "            self.use_pca = False\n",
    "            self.processed_data_original = processed_data\n",
    "        \n",
    "        print(f\"Final processed data shape: {self.processed_data.shape}\")\n",
    "        return self.processed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1994c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Enhanced usage function with comprehensive analysis\n",
    "def run_enhanced_clustering_analysis(df, categorical_cols=None, numerical_cols=None, \n",
    "                                    use_pca=True, target_clusters=None):\n",
    "    \"\"\"\n",
    "    Main function to run enhanced clustering analysis with all improvements\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting Enhanced Demographic Clustering Analysis\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Initialize clusterer\n",
    "    clusterer = DemographicClusterer(df=df)\n",
    "    \n",
    "    # Enhanced preprocessing with composite features\n",
    "    clusterer.preprocess_data(categorical_cols=categorical_cols, \n",
    "                            numerical_cols=numerical_cols, \n",
    "                            use_pca=use_pca)\n",
    "    \n",
    "    # Find optimal number of clusters using multiple methods\n",
    "    print(f\"\\nüìä Finding optimal number of clusters...\")\n",
    "    optimal_k = clusterer.find_optimal_clusters(max_clusters=8)\n",
    "    \n",
    "    if target_clusters is not None:\n",
    "        optimal_k = target_clusters\n",
    "        print(f\"Using specified target clusters: {optimal_k}\")\n",
    "    \n",
    "    # Apply enhanced clustering methods (HDBSCAN priority)\n",
    "    print(f\"\\nüîç Applying clustering algorithms...\")\n",
    "    results = clusterer.apply_clustering_methods(n_clusters=optimal_k)\n",
    "    \n",
    "    # Comprehensive evaluation\n",
    "    print(f\"\\nüìà Evaluating clustering quality...\")\n",
    "    evaluation = clusterer.evaluate_clusters()\n",
    "    \n",
    "    # Find best method based on overall score\n",
    "    best_method = 'HDBSCAN'  # Default to HDBSCAN\n",
    "    if not evaluation.empty:\n",
    "        best_row = evaluation.loc[evaluation['Overall Score'].astype(float).idxmax()]\n",
    "        best_method = best_row['Method']\n",
    "    \n",
    "    print(f\"\\nüéØ Primary analysis using: {best_method}\")\n",
    "    \n",
    "    # Advanced visualizations and analysis\n",
    "    methods_to_analyze = [best_method]\n",
    "    if best_method != 'HDBSCAN' and 'HDBSCAN' in results:\n",
    "        methods_to_analyze.append('HDBSCAN')\n",
    "    if best_method != 'K-Means' and 'K-Means' in results:\n",
    "        methods_to_analyze.append('K-Means')\n",
    "    \n",
    "    for method in methods_to_analyze:\n",
    "        if method in results:\n",
    "            print(f\"\\n\" + \"=\"*50)\n",
    "            print(f\"DETAILED ANALYSIS: {method}\")\n",
    "            print(\"=\"*50)\n",
    "            \n",
    "            # 1. Create cluster profiles\n",
    "            print(f\"\\nüìã Creating cluster profiles...\")\n",
    "            df_with_clusters = clusterer.create_cluster_profiles(method)\n",
    "            \n",
    "            # 2. Generate segment names\n",
    "            print(f\"\\nüè∑Ô∏è Generating segment names...\")\n",
    "            segment_names = clusterer.generate_segment_names(method)\n",
    "            \n",
    "            # 3. Advanced t-SNE visualization\n",
    "            print(f\"\\nüìä Creating advanced t-SNE visualizations...\")\n",
    "            clusterer.create_advanced_tsne_visualization(method, perplexity_values=[30, 50])\n",
    "            \n",
    "            # 4. Enhanced radial charts\n",
    "            print(f\"\\nüé≠ Creating enhanced radial charts...\")\n",
    "            clusterer.create_enhanced_radial_charts(method, show_comparison=True)\n",
    "            \n",
    "            # 5. Feature contribution analysis\n",
    "            print(f\"\\nüîç Analyzing feature contributions...\")\n",
    "            clusterer.create_feature_contribution_plot(method)\n",
    "            \n",
    "            # 6. Business dashboard\n",
    "            print(f\"\\nüìà Creating business dashboard...\")\n",
    "            clusterer.create_business_dashboard(method)\n",
    "            \n",
    "            # 7. Comparison heatmap\n",
    "            print(f\"\\nüå°Ô∏è Creating comparison heatmap...\")\n",
    "            clusterer.create_comparison_heatmap(method)\n",
    "            \n",
    "            print(f\"\\n\" + \"-\"*50)\n",
    "    \n",
    "    # Stability analysis across methods\n",
    "    print(f\"\\nüî¨ Performing stability analysis...\")\n",
    "    stability_results = clusterer.create_cluster_stability_analysis(methods=list(results.keys())[:3])\n",
    "    \n",
    "    # Final recommendations\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"üéØ FINAL RECOMMENDATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"Best Method: {best_method}\")\n",
    "    if best_method in results:\n",
    "        best_result = results[best_method]\n",
    "        print(f\"Number of Clusters: {best_result['n_clusters']}\")\n",
    "        if 'silhouette_score' in best_result:\n",
    "            print(f\"Silhouette Score: {best_result['silhouette_score']:.3f}\")\n",
    "        if 'noise_points' in best_result:\n",
    "            print(f\"Noise Points: {best_result['noise_points']}\")\n",
    "    \n",
    "    print(f\"\\nüí° Business Insights:\")\n",
    "    if best_method in results:\n",
    "        labels = results[best_method]['labels']\n",
    "        valid_clusters = len(set(labels[labels >= 0]))\n",
    "        \n",
    "        if valid_clusters >= 4 and valid_clusters <= 6:\n",
    "            print(\"  ‚úì Optimal number of segments for business strategy\")\n",
    "        elif valid_clusters < 4:\n",
    "            print(\"  ‚ö† Consider increasing clusters for more granular insights\")\n",
    "        else:\n",
    "            print(\"  ‚ö† Consider reducing clusters for simpler strategy\")\n",
    "        \n",
    "        # Check if we have the target segments\n",
    "        df_with_clusters = df.copy()\n",
    "        df_with_clusters['Cluster'] = labels\n",
    "        df_filtered = df_with_clusters[df_with_clusters['Cluster'] >= 0]\n",
    "        \n",
    "        if len(df_filtered) > 0:\n",
    "            min_cluster_size = df_filtered['Cluster'].value_counts().min()\n",
    "            min_pct = (min_cluster_size / len(df_filtered)) * 100\n",
    "            \n",
    "            if min_pct >= 5:\n",
    "                print(\"  ‚úì All segments are actionable (>5% of population)\")\n",
    "            else:\n",
    "                print(\"  ‚ö† Some segments may be too small for targeted strategies\")\n",
    "    \n",
    "    print(f\"\\nüéØ Next Steps:\")\n",
    "    print(\"  1. Review segment characteristics and create detailed personas\")\n",
    "    print(\"  2. Develop targeted marketing strategies for each segment\")\n",
    "    print(\"  3. Validate segments with business stakeholders\")\n",
    "    print(\"  4. Monitor segment stability over time\")\n",
    "    print(\"  5. Consider A/B testing different approaches per segment\")\n",
    "    \n",
    "    return clusterer, results, best_method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eccd40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e327b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run analysis on enhanced sample data\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Creating enhanced sample demographic data...\")\n",
    "    sample_df = create_enhanced_sample_demographic_data(1000)\n",
    "    print(\"Enhanced sample data created successfully!\")\n",
    "    print(f\"Data shape: {sample_df.shape}\")\n",
    "    print(\"\\nSample data preview:\")\n",
    "    print(sample_df.head())\n",
    "    print(\"\\nData info:\")\n",
    "    print(sample_df.info())\n",
    "    \n",
    "    # Run enhanced clustering analysis\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STARTING ENHANCED CLUSTERING ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    clusterer, results, best_method = run_enhanced_clustering_analysis(\n",
    "        sample_df, \n",
    "        use_pca=True, \n",
    "        target_clusters=5  # Target the 5 business segments\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf472340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (10695, 8)\n",
      "Data columns: ['Gender', 'Ever_Married', 'Age', 'Graduated', 'Profession', 'Work_Experience', 'Spending_Score', 'Family_Size']\n",
      "Data types:\n",
      "Gender              object\n",
      "Ever_Married        object\n",
      "Age                  int64\n",
      "Graduated           object\n",
      "Profession          object\n",
      "Work_Experience    float64\n",
      "Spending_Score      object\n",
      "Family_Size        float64\n",
      "dtype: object\n",
      "Missing values:\n",
      "Gender             0\n",
      "Ever_Married       0\n",
      "Age                0\n",
      "Graduated          0\n",
      "Profession         0\n",
      "Work_Experience    0\n",
      "Spending_Score     0\n",
      "Family_Size        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "datapath = \"/Users/whysocurious/Documents/MLDSAIProjects/cust-seg-case-study/data\"\n",
    "dfcust = pd.read_csv(datapath+'/customer_data_imputed.csv').set_index('Customer ID')\n",
    "#(datapath+'/Customer Segmentation.csv')\n",
    "print (f\"Data shape: {dfcust.shape}\")\n",
    "print (f\"Data columns: {dfcust.columns.tolist()}\")\n",
    "print (f\"Data types:\\n{dfcust.dtypes}\")\n",
    "print (f\"Missing values:\\n{dfcust.isnull().sum()}\")\n",
    "# dfcust.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbedbea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Enhanced Demographic Clustering Analysis\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DemographicClusterer' object has no attribute 'preprocess_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m clusterer, results, best_method \u001b[38;5;241m=\u001b[39m \u001b[43mrun_enhanced_clustering_analysis\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdfcust\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_pca\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Target the 5 business segments\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m, in \u001b[0;36mrun_enhanced_clustering_analysis\u001b[0;34m(df, categorical_cols, numerical_cols, use_pca, target_clusters)\u001b[0m\n\u001b[1;32m     11\u001b[0m clusterer \u001b[38;5;241m=\u001b[39m DemographicClusterer(df\u001b[38;5;241m=\u001b[39mdf)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Enhanced preprocessing with composite features\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mclusterer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_data\u001b[49m(categorical_cols\u001b[38;5;241m=\u001b[39mcategorical_cols, \n\u001b[1;32m     15\u001b[0m                         numerical_cols\u001b[38;5;241m=\u001b[39mnumerical_cols, \n\u001b[1;32m     16\u001b[0m                         use_pca\u001b[38;5;241m=\u001b[39muse_pca)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Find optimal number of clusters using multiple methods\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müìä Finding optimal number of clusters...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DemographicClusterer' object has no attribute 'preprocess_data'"
     ]
    }
   ],
   "source": [
    "clusterer, results, best_method = run_enhanced_clustering_analysis(\n",
    "        dfcust, \n",
    "        use_pca=True, \n",
    "        target_clusters=5  # Target the 5 business segments\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2947062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ae2a19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cust-seg-case-study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
